{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9b3d4cd9-36b5-9f1a-071e-7b2f79188e08","_uuid":"4bc611381156b69d9c07743cd52f17736a7b1501"},"source":["**Introduction**\n","\n","From Wikipedia\n","\n","The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species.[2] Two of the three species were collected in the Gasp√© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".[3]\n","\n","The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n","\n","The iris dataset contains measurements for 150 iris flowers from three different species.\n","\n","The three classes in the Iris dataset:\n","\n","    Iris-setosa (n=50). \n","    Iris-versicolor (n=50).\n","    Iris-virginica (n=50). \n","\n","The four features of the Iris dataset:\n","\n","    sepal length in cm\n","    sepal width in cm\n","    petal length in cm\n","    petal width in cm"]},{"cell_type":"markdown","metadata":{"_uuid":"0e48167ac1a11dc93818a3afccb7f11955476001"},"source":["For this classification exercise on the Iris species data, \n","* I first use some simple Python techniques to explore the data set. \n","* Then I split half of the data into the training set to train the hypothesis model and half of them as validation set to check the test accuracy score \n","* At last, we use the support vector machine to train the classification model.\n","* And we use GridSearchCV to tune the hyperparameters(C, gamma, kernel) in the SVC model to achieve 100% accuracy score.\n","\n","One very import reminder is below. train_test_split shuffle the data before doing the split.  GridSearchCV does not shuffle the data before doing cross-validation. And our iris data is ordered by response variable Species ( 50 Iris-setosa, 50 Iris-versicolor, and 50 Iris-virginica)  so we need to shuffle iris before using GridSearchCV.\n","\n","In the future, I will also add the decision tree, bagging, Boosting and AdaBoost,  random forest classifier, logistic regression, K nearest neighbor classifier, naive Bayes to classify the model. "]},{"cell_type":"markdown","metadata":{"_uuid":"69f83238d75c342ede3e08871985279e425679eb"},"source":["* If you think my kernel is helpful, please give me a voteup. This is very important for new people like me. Thank you in advance.\n","* If you have any question, please feel free to leave me a message, I will check every day. Thank you so much."]},{"cell_type":"markdown","metadata":{"_uuid":"e6b110976674f41c70e5c3ec57f69153acda0fdb"},"source":["**Part I: Import library and load data**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8321b3ea-0b13-b961-5b3b-050e5037c2a1","_uuid":"946bbe0b2f194a3d8905796a90078963f19cfe52","trusted":true},"outputs":[],"source":["# Data analysis libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Data visualization libraires\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# show plot in the notebook\n","%matplotlib inline\n","\n","# Next, we'll load the Iris flower dataset, which is in the \"../input/\" directory\n","iris = pd.read_csv(\"dataset\\Iris.csv\") # the iris dataset is now a Pandas DataFrame"]},{"cell_type":"markdown","metadata":{"_uuid":"3bd2939c68298960bfe149b45ecdda92af83820c"},"source":["**Part II: Check the data information**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"096c59a9-7815-e28c-439c-59d802ffa664","_uuid":"c957539b946699b7e099f875855baad3523af874","trusted":true},"outputs":[],"source":["# first five observations\n","iris.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"508b1d47-4645-d95f-f196-e87b68656d28","_uuid":"6df13465312259290f51d769fc4bf63081a0334c","trusted":true},"outputs":[],"source":["# Number of observations and missing values. \n","# There are 150 observations and no nan value\n","iris.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2823cb4-04bb-e519-7771-6f45d24babc8","_uuid":"54e39af16ab8f0e0d1af08252f0a8b0bbb456d70","trusted":true},"outputs":[],"source":["# Check basic description for features\n","iris.drop(['Id','Species'], axis=1).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fe9bf6d-06cb-c5ae-6875-073202653937","_uuid":"0789898ff2f119369f19166cec03b9b1b00ac55c","trusted":true},"outputs":[],"source":["# Check the response variable frequency\n","iris['Species'].value_counts()"]},{"cell_type":"markdown","metadata":{"_uuid":"e2d84cea6b09c4e1faf681d89f67f0134066ac55"},"source":["**Part III: Explorary data analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b25bcbe-dc0e-f762-dbfa-dfd44515f0ac","_uuid":"bb43c6b2ec077bbe2c334903e9ba1eafc0f8f293","trusted":true},"outputs":[],"source":["# Create a pairplot of the data set. Which flower species seems to be the most separable?\n","sns.pairplot(iris.drop(['Id'], axis=1),hue='Species')\n","# Iris setosa seems most separable from the other two species"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c6e74ab-85c4-0f63-d432-900a4f907691","_uuid":"8c4ccd9c01e2699113060fc3467af34d71ed18ba","trusted":true},"outputs":[],"source":["# Create a kde plot of sepal_length versus sepal width for setosa species of flower.\n","sub=iris[iris['Species']=='Iris-setosa']\n","sns.kdeplot(data=sub[['SepalLengthCm','SepalWidthCm']],cmap=\"plasma\", shade=True, shade_lowest=False)\n","plt.title('Iris-setosa')\n","plt.xlabel('Sepal Length Cm')\n","plt.ylabel('Sepal Width Cm')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"80aaa2afcccf81b77bd8170705797d68344e3b35","trusted":true},"outputs":[],"source":["sns.kdeplot(data=sub[['PetalLengthCm','PetalWidthCm']],cmap=\"plasma\", shade=True, shade_lowest=False)\n","plt.title('Iris-setosa')\n","plt.xlabel('Petal Length Cm')\n","plt.ylabel('Petal Width Cm')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4bf1b4bc1c2ce1c4267a69dccb30f097a72019aa","trusted":true},"outputs":[],"source":["sub_virginica=iris[iris['Species']=='Iris-virginica']\n","# Create a scatter plot of the Sepal\n","plt.scatter(sub_virginica['SepalLengthCm'], sub_virginica['SepalWidthCm'], marker='o', color='r')\n","plt.xlabel('Sepal Length Cm')\n","plt.ylabel('Sepal Width Cm')\n","plt.title('Sepal Width versus Length for virginica species')"]},{"cell_type":"markdown","metadata":{"_uuid":"67adc875d3f899c70329ce30ac3dd7351a3b4eb3"},"source":["**Part 4: Train Test Split**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24b14fd2-c282-2e91-a700-3cf5f7019831","_uuid":"15d4b1e9c4127be8a9222670e5eaaf0e078c13a3","trusted":true},"outputs":[],"source":["# Split data into a training set and a testing set.\n","# train_test_split shuffle the data before the split (shuffle=True by default)\n","from sklearn.model_selection import train_test_split\n","X=iris.drop(['Species', 'Id'], axis=1)\n","y=iris['Species']\n","X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.5, shuffle=True,random_state=100)"]},{"cell_type":"markdown","metadata":{"_uuid":"35f993a87ead7968d9dd9d3ea3cda21c9f138e94"},"source":["**Part 5: Train a Model**\n","\n","For sklearn.svm.SVC(), The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e67e090-4435-4d69-cdfc-b820112a064c","_uuid":"eab549f431671de093f96d8c4f6f0c3dc0a0c759","trusted":true},"outputs":[],"source":["# Now it's time to train a Support Vector Machine Classifier. \n","# Call the SVC() model from sklearn and fit the model to the training data.\n","from sklearn.svm import SVC\n","model=SVC(C=1, kernel='rbf', tol=0.001)\n","model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"_uuid":"9fbee5b641bb65ceed50023328e5d3f3d32f6592"},"source":["**Part 6: Model Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6527e9e7-61be-05e3-f986-fdd4084c1aa8","_uuid":"c06d72a804e069ad8c0028a7163401c301ffa04f","trusted":true},"outputs":[],"source":["# Now get predictions from the model and create a confusion matrix and a classification report.\n","pred=model.predict(X_test)\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","print(confusion_matrix(y_test, pred))\n","print('\\n')\n","print(classification_report(y_test, pred))\n","print('\\n')\n","print('Accuracy score is: ', accuracy_score(y_test, pred))"]},{"cell_type":"markdown","metadata":{"_uuid":"cc74bd38d431cff1c53d7f34c9ed3e28376d2abf"},"source":["**Part 7: Gridsearch to tune hyperparameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9077671b6e1c1ebb7982e85301fdd4ba3e15838e","trusted":true},"outputs":[],"source":["iris.head(20)"]},{"cell_type":"markdown","metadata":{"_uuid":"cdc47493f67cfa49a1e3131e4105e047ae4bf9e5"},"source":["***  7.1 GridSearchCV does not shuffle the data before CV like train_test_split, we need to shuttle the Iris data by ourselves before using the GridSearchCV. Since in the original Iris data, data is sorted by species. ***"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2da4df4da1c02dcbb69af9614e9640a1996111df","trusted":true},"outputs":[],"source":["from sklearn.utils import shuffle\n","X=iris.drop('Species', axis=1)\n","y=iris['Species']\n","print('Before shuffle: ',y[0:20])\n","X,y = shuffle(X,y, random_state=0)\n","print(\"After shuffle: \", y[0:20])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3a6054c-d87c-1219-8b5c-ad76fc99b70a","_uuid":"e96d2be6f97b627ea2663e110747621a3daef120","trusted":true},"outputs":[],"source":["# Create a dictionary called param_grid and fill out some parameters for C and gamma.\n","param_grid = {'C': [0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n","# param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': ['auto'], 'kernel': ['rbf']}\n","from sklearn.model_selection import GridSearchCV\n","grid=GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy',cv=3, verbose=1, refit=True )\n","grid.fit(X, y)"]},{"cell_type":"markdown","metadata":{"_uuid":"2f1e0dd1af250002d080aa04ef2274625bd77334"},"source":["* The GridSearchCV exhaustive search over specified parameter values for an estimator in param_grid.\n","* CV=3 means we will use the three-fold cross-validation and check the performance of the mean accuracy score on the validation set (default is cv=3, three-fold cross-validation)\n","* scoring='accuracy' means we choose parameters with the best accuracy score, this is the default setting. We can also use scoring='precision', 'f1', 'recall'\n","* By using GridSearchCV to tune hyperparameters, we get 98.6% test accuracy score."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"869711c0-67e1-4e86-adfa-78defe99141c","_uuid":"5607fd6176bc6a4db2347759cd28489152f3c869","trusted":true},"outputs":[],"source":["# The best hyperparameters chosen is\n","print(grid.best_params_)\n","print(grid.best_estimator_)\n","print('Mean cross-validated score of the best_estimator: ', grid.best_score_)\n","print('The number of cross-validation splits (folds/iterations): ', grid.n_splits_)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"43f4e9bdc1e5d906b6eecccf21b4fa4c511f2b09","trusted":true},"outputs":[],"source":["# Another optition for shuffle is to use cv=KFold, we get 98% accuracy\n","from sklearn.model_selection import KFold\n","X=iris.drop(['Species', 'Id'], axis=1)\n","y=iris['Species']\n","# Create a dictionary called param_grid and fill out some parameters for C and gamma.\n","param_grid = {'C': [0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n","# param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': ['auto'], 'kernel': ['rbf']}\n","from sklearn.model_selection import GridSearchCV\n","grid=GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy',\n","                  cv=KFold(n_splits=3, shuffle=True, random_state=0), verbose=1, refit=True )\n","grid.fit(X, y)\n","\n","# The best hyperparameters chosen is\n","print(grid.best_params_)\n","print(grid.best_estimator_)\n","print('Mean cross-validated score of the best_estimator: ', grid.best_score_)\n","print('The number of cross-validation splits (folds/iterations): ', grid.n_splits_)"]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}
